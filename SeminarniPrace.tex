\documentclass[12pt, a4paper]{article}

% --- ZÁKLADNÍ BALÍČKY ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[czech]{babel}
\usepackage{lmodern}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{csquotes}
\usepackage{geometry}
\usepackage{makecell}

% --- OSTATNÍ BALÍČKY ---
\usepackage[numbers]{natbib}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{enumitem}
\pagecolor{white} 
\color{black}

% --- NASTAVENÍ STRÁNKY ---
\geometry{
    a4paper,
    top=2.5cm,
    bottom=2.5cm,
    left=1.5cm,
    right=1.5cm
}

% --- NASTAVENÍ ZDROJOVÉHO KÓDU ---
\definecolor{codegray}{gray}{0.9}
\lstset{
    backgroundcolor=\color{codegray},
    basicstyle=\ttfamily\small,
    breaklines=true,
    captionpos=b,
    tabsize=4
}

% --- INFORMACE O DOKUMENTU ---
\title{Code review pomocí velkého jazykového modelu}
\author{Valdemar Pospíšil}
\date{Květen 2025}

\begin{document}

\maketitle

\begin{abstract}
Tato práce se zabývá využitím velkých jazykových modelů (LLM) při procesu kontroly zdrojového kódu, tzv. \textit{code review}, ve vývoji softwaru. Cílem je prozkoumat možnosti, přínosy a limity těchto modelů v reálném vývojářském workflow a navrhnout experimenty, které ověří jejich efektivitu ve srovnání s lidskými recenzenty.
\end{abstract}

\section{Úvod do tématu}
V současném softwarovém vývoji představuje \textit{code review} nedílnou součást procesu zajišťující kvalitu zdrojového kódu. Slouží nejen k odhalování chyb, ale i k předávání znalostí mezi členy týmu, udržování konzistentního stylu kódu a zvyšování celkové udržovatelnosti systému. Tato praxe je klíčová zejména ve větších týmech a projektech s dlouhodobým vývojem. V posledních letech se do vývojářského procesu stále více zapojují nástroje založené na umělé inteligenci. Jedním z nejvýraznějších pokroků v této oblasti jsou tzv. \textit{velké jazykové modely} (LLM – Large Language Models), jako je ChatGPT, Claude, Gemini nebo GitHub Copilot. Tyto modely dokáží porozumět strukturovanému i nestrukturovanému textu a generovat smysluplné odpovědi, komentáře nebo návrhy na základě vstupních dat. Otázkou tedy je, do jaké míry lze tyto nástroje využít pro automatizaci nebo podporu code review. Může LLM odhalit stejné chyby jako zkušený programátor? Je jeho návrh na refaktoring použitelný v reálném prostředí? A především – může takový model plnohodnotně doplnit, nebo dokonce nahradit lidského recenzenta? Tato seminární práce si klade za cíl popsat současný stav výzkumu v této oblasti, formulovat výzkumné otázky a navrhnout experiment, který pomůže zodpovědět, jak efektivní je využití LLM při provádění code review.
\newpage

\section{State-of-the-art}
V současné době dochází k výraznému průniku nástrojů umělé inteligence do procesu vývoje softwaru, včetně code review. Tato sekce představuje stručný přehled aktuálního stavu výzkumu s důrazem na oblasti relevantní pro naše výzkumné otázky.

\subsection{Výhody a nevýhody lidského code review}
Tradiční procesy revize kódu, ačkoliv jsou zásadní pro udržení kvality softwaru a sdílení znalostí v týmu \cite{zdrojak2022}, čelí několika významným výzvám. Mezi hlavní nevýhody lidského code review patří především časová náročnost, možnost lidské chyby a nekonzistence hodnocení, která může pramenit z odlišných zkušeností a standardů jednotlivých revidentů. Jak uvádí Falcon \cite{falcon2024devto}, tyto nedostatky mohou vést ke zpoždění v rámci agilních vývojových cyklů, jako je kontinuální integrace a nasazování (CI/CD), a k nedostatečnému odhalení specifických technických problémů, pokud revidující postrádá hlubší znalost konkrétní technologie.

Na druhou stranu, lidské code review přináší nezpochybnitelné výhody. Foster \cite{graphite2023} zdůrazňuje, že lidští recenzenti excelují v porozumění širšímu kontextu aplikace, dokáží identifikovat subtilní architektonické problémy a zohledňovat specifické požadavky projektu či organizace. Navíc tento proces slouží jako prostředek ke sdílení znalostí a mentoringu v týmu, což je aspekt, který automatizované nástroje nemohou plně nahradit.

\subsection{Výhody a nevýhody LLM code review}
V reakci na limity lidského code review se do popředí dostává potenciál umělé inteligence. AI nástroje, zejména velké jazykové modely, slibují automatizaci určitých aspektů revize kódu. Dle Falcona \cite{falcon2024devto} mohou LLM provádět statickou analýzu kódu k identifikaci běžných syntaktických chyb, stylistických prohřešků, potenciálních bezpečnostních zranitelností či použití zastaralých částí kódu. Dále mohou navrhovat vylepšení směřující k lepší čitelnosti, efektivitě a udržovatelnosti kódu v souladu s osvědčenými programátorskými postupy. AI je také schopna detekovat anomálie a odchylky od zavedených týmových konvencí a v neposlední řadě může usnadnit práci lidským revidentům tím, že provede prvotní kontrolu a upozorní na klíčové oblasti vyžadující podrobnější lidské posouzení.

Navzdory těmto výhodám, Foster \cite{graphite2023} identifikuje několik klíčových limitací LLM při code review. Mezi tyto nevýhody patří omezené chápání kontextu celé aplikace, problém s halucinacemi (generování přesvědčivě znějících, ale fakticky nesprávných informací) a zejména tzv. "nekritická pasivnost", kdy modely nejsou schopny rozpoznat subtilní designové problémy. Dalším významným omezením je absence porozumění specifickým potřebám projektu a organizačním standardům, které nejsou explicitně vyjádřeny v kódu samotném.

\subsection{Praktické implementace v reálném prostředí}
Příklad praktického nasazení LLM pro code review popisuje Bjerring \cite{bjerring2024automated} na implementaci ve společnosti Faire. Ta vyvinula orchestrátorovou službu \textit{Fairey}, která propojuje GitHub webhooky s OpenAI Assistants API a využívá techniku RAG (Retrieval Augmented Generation) pro získání kontextu specifického pro daný pull request. Tato architektura umožňuje automatické spouštění review při splnění kritérií jako je jazyk kódu nebo obsah změn.

Klíčovým přínosem této integrace je snížení latence v procesu review. LLM dokáží rychle zpracovat rutinní úkoly, zatímco lidští recenzenti se mohou soustředit na komplexnější problémy vyžadující hlubší kontext. Zkušenosti Faire demonstrují, že i když LLM nenahradí lidské recenzenty v oblastech jako architektonické rozhodování, jejich role v automatizaci rutinních kontrol se stává významným doplňkem vývojového procesu.

\subsection{Nástroje a technologie pro automatizované code review}
V současné době existuje několik způsobů, jak využít LLM pro code review v různých fázích vývojového procesu. Foster \cite{graphite2023} popisuje jednoduchý, ale účinný přístup pro ad-hoc code review: k URL adrese pull requestu na GitHubu stačí přidat příponu \texttt{.diff}, zkopírovat výsledný diff soubor a vložit ho do libovolného chatovacího LLM jako ChatGPT, Claude nebo Gemini. Tento přístup je limitován kontextovým oknem daného modelu, ale poskytuje rychlou zpětnou vazbu bez nutnosti specializovaných nástrojů.

Pro systematičtější integraci do vývojového procesu Falcon \cite{falcon2024devto} představuje řešení založené na kombinaci git hooks a Code Llama modelu běžícího v Docker kontejneru. Jeho implementace spočívá ve vytvoření pre-commit hooku, který automaticky spouští code review pro všechny modifikované Python soubory před dokončením commitu. Tento přístup nabízí několik výhod:

\begin{itemize}
  \item Okamžitá zpětná vazba ještě před odesláním kódu do repozitáře
  \item Konzistentní kontrola kódu pro každou změnu
  \item Automatizovaná dokumentace doporučení v markdown formátu
  \item Možnost lokálního běhu bez závislosti na externích službách
\end{itemize}

Vedle těchto přístupů existují i integrovaná řešení jako GitHub Copilot \cite{copilot2023}, který poskytuje code review přímo v prostředí GitHub pull requestů, nebo samostatné nástroje jako Code Rabbit, které se automaticky aktivují při vytvoření pull requestu. Tyto nástroje často využívají pokročilé techniky jako je RAG (Retrieval Augmented Generation) pro lepší porozumění kontextu kódu a poskytují strukturovanější a relevantnější zpětnou vazbu než obecné chatovací modely.


\section{Výzkumné otázky}
V rámci této práce se zaměřím na následující výzkumné otázky:
\begin{itemize}
\item \textbf{Má nekritická pasivnost vliv na kvalitu code review?}\
Nekritická pasivnost představuje tendenci LLM vyhýbat se kritickým hodnocením a přílišné důvěře v předložený kód. Tato otázka zkoumá, do jaké míry tento fenomén ovlivňuje kvalitu a užitečnost automatizovaného code review ve srovnání s lidskými recenzenty.
\item \textbf{Jak dobře si LLM poradí s review kódu v méně běžném jazyce jako je Haskell?}\
Zaměřím se výhradně na Haskell, jelikož jde o méně používaný funkcionální programovací jazyk s odlišným paradigmatem než běžnější imperativní jazyky. Tato volba je zajímavá především proto, že na internetu existuje znatelně méně zdrojových kódů v Haskellu oproti jazykům jako Python, Java nebo JavaScript. To může potenciálně znamenat, že LLM měly během svého trénování k dispozici méně příkladů a best practices specifických pro Haskell, což by mohlo vést k méně kvalitním výsledkům code review pro tento jazyk.
\item \textbf{Ovlivňuje jazyk instrukcí (čeština vs. angličtina vs. velština) kvalitu code review provedeného LLM?}\
Tato otázka zkoumá, zda jazyk, v němž jsou formulovány instrukce pro LLM, má vliv na kvalitu poskytnutého code review. Přestože velké jazykové modely jsou prezentovány jako multilingvální nástroje, jejich primární trénovací data jsou často převážně v angličtině. Je tedy relevantní zkoumat, zda při použití českých nebo dokonce velšských promptů (jako příklad vzácného jazyka s malým zastoupením v trénovacích datech) dochází ke snížení kvality analýzy kódu ve srovnání s anglickými instrukcemi. Tato volba jazyků umožňuje testovat hypotézu, že čím méně četný je jazyk v trénovacích datech, tím horší je kvalita code review při použití tohoto jazyka pro zadání instrukcí.

\end{itemize}

\section{Návrh experimentu}
Pro zodpovězení stanovených výzkumných otázek jsem připravil komplexní experiment založený na systematickém testování vybraných LLM modelů. Experiment byl navržen tak, aby umožnil kvantitativní i kvalitativní hodnocení schopnosti různých modelů provádět code review za různých podmínek a pro různé programovací jazyky.
\subsection{Příprava testovacího prostředí}
Pro účely experimentu jsem připravil dvě hlavní sady zdrojových kódů:
\subsubsection{Python TaskManager}
Pro zkoumání nekritické pasivnosti a vlivu jazyka instrukce byl vybrán konkrétní softwarový projekt - jednoduchý správce úkolů (TaskManager) implementovaný v jazyce Python \cite{pospisil2025}. Tento projekt byl zvolen z několika důvodů:
\begin{itemize}
\item Přiměřená komplexita - kód je dostatečně rozsáhlý, aby obsahoval různé typy problémů, ale zároveň není příliš komplexní, což by mohlo vést k nepřehlednosti při hodnocení.
\item Obecně srozumitelná doména - správa úkolů je intuitivně pochopitelná oblast, což minimalizuje potřebu dodatečného kontextu.
\item Možnost záměrného vložení různých typů chyb - od zjevných až po subtilní.
\end{itemize}
Do kódu byly záměrně vloženy následující problémy:
\begin{itemize}
\item 4 zjevné problémy - snadno odhalitelné chyby, které by měl identifikovat i méně zkušený programátor nebo základní statická analýza
\item 4 středně závažné problémy - vyžadující hlubší analýzu kódu, ale stále poměrně dobře identifikovatelné
\item 6 subtilních problémů - vyžadující hlubší zamyšlení, znalost kontextu nebo pokročilou znalost programovacích praktik
\end{itemize}
Pro účely experimentu zkoumajícího vliv jazyka instrukce byly připraveny tři jazykové verze promptů - česká, anglická a velšská.
\subsubsection{Haskell HaikuWhisperer}
Pro zkoumání schopnosti LLM provádět code review v méně běžném programovacím jazyce byl vybrán projekt HaikuWhisperer \cite{pospisil2025haiku}. Tento projekt implementuje generátor haiku básní v jazyce Haskell a paralelně existuje i jeho implementace v Pythonu. Důvodem pro výběr tohoto projektu bylo:
\begin{itemize}
\item Možnost přímého srovnání dvou implementací stejné funkcionality v odlišných programovacích jazycích
\item Haskell jako reprezentant funkcionálního programovacího paradigmatu, který je méně zastoupený na internetu
\item Relativně jednoduchá funkcionalita, která je však implementována s využitím specifických vlastností obou jazyků
\end{itemize}
Pro experiment s Haskellem bude využito dvou přístupů:
\begin{itemize}
\item Použití služby CodeRabbit jako automatizovaného nástroje pro code review
\item Manuální přidání přípony .diff k URL adrese pull requestu a následné vložení obsahu do chatovacích modelů
\end{itemize}
\subsection{Metodika testování LLM}
Pro testování byly vybrány čtyři široce používané velké jazykové modely v jejich nejmodernějších verzích:
\begin{itemize}
\item Claude 3.7 Sonnet (Anthropic) \cite{claude2025}
\item ChatGPT-4o (OpenAI) \cite{chatgpt2025}
\item Gemini 2.5 Pro (Google) \cite{gemini2025}
\item DeepSeek R-1 (DeepSeek AI) \cite{deepseek2025}
\end{itemize}
Pro službu CodeRabbit není veřejně dostupná informace o konkrétním použitém modelu.
\subsection{Typy promptů pro jednotlivé výzkumné otázky}
\subsubsection{Prompty pro zkoumání nekritické pasivnosti}
Pro účely zkoumání vlivu nekritické pasivnosti na kvalitu code review byly pro každý model připraveny tři různé typy promptů:
\begin{itemize}
\item \textbf{Neutrální prompt:}
\begin{verbatim}
Proveď podrobné code review následujícího Python kódu pro správu úkolů
(Task Manager). Zhodnoť architektonické rozhodnutí,
identifikuj potenciální problémy a navrhni případná vylepšení.
[zde vložen kód]
\end{verbatim}
\item \textbf{Pozitivní prompt:}
\begin{verbatim}
Proveď code review následujícího Python kódu pro správu úkolů
(Task Manager). Tento kód byl vytvořen zkušeným vývojářem podle
moderních standardů a byl už částečně optimalizován.
Zdá se být dobře strukturovaný, ale uvítám tvůj pohled
na něj. Pokud najdeš nějaké drobné optimalizace, určitě je zmiň.
[zde vložen kód]
\end{verbatim}
\item \textbf{Negativní prompt:}
\begin{verbatim}
Proveď důkladné a kritické code review následujícího Python kódu
pro správu úkolů
(Task Manager). Tento kód obsahuje několik chyb, neefektivních částí
a porušuje některé best practices. Identifikuj co nejvíce problémů,
včetně závažných i méně závažných, a navrhni, jak by měly být opraveny.
Buď prosím přísný ve svém hodnocení.
[zde vložen kód]
\end{verbatim}
\end{itemize}
\subsubsection{Prompty pro zkoumání vlivu jazyka instrukce}
Pro experiment zkoumající vliv jazyka instrukcí byly připraveny tři jazykové verze neutrálního promptu:
\begin{itemize}
\item \textbf{Anglický prompt:}
\begin{verbatim}
Perform a detailed code review of the following Python code for task management
(Task Manager). Evaluate architectural decisions, identify potential issues,
and suggest possible improvements.
[code inserted here]
\end{verbatim}
\item \textbf{Český prompt:} (Totožný s neutrálním promptem uvedeným výše)
\item \textbf{Velšský prompt:}
\begin{verbatim}
Cyflawnwch adolygiad cod manwl o'r cod Python canlynol ar gyfer rheoli tasgau
(Task Manager). Gwerthuswch benderfyniadau pensaernïol, adnabod problemau posibl,
ac awgrymu gwelliannau posibl.
[code wedi'i fewnosod yma]
\end{verbatim}
\end{itemize}
\subsubsection{Prompty pro zkoumání schopnosti review kódu v Haskellu}
Pro experiment s Haskellem byly připraveny dva typy promptů:
\begin{itemize}
\item \textbf{Prompt pro Haskell implementaci:}
\begin{verbatim}
Proveď podrobné code review následujícího Haskell kódu pro generátor haiku.
Zaměř se na idiomatické použití Haskellu, funkcionální programování,
typovou bezpečnost a celkovou efektivitu. Identifikuj případné problémy
a navrhni vylepšení.
[zde vložen Haskell kód]
\end{verbatim}
\item \textbf{Prompt pro Python implementaci (pro srovnání):}
\begin{verbatim}
Proveď podrobné code review následujícího Python kódu pro generátor haiku.
Zaměř se na efektivitu, čitelnost, dodržování best practices
a celkovou architekturu. Identifikuj případné problémy a navrhni vylepšení.
[zde vložen Python kód]
\end{verbatim}
\end{itemize}
\subsection{Metriky hodnocení}
Pro objektivní vyhodnocení výstupů z jednotlivých modelů a typů promptů jsem stanovil následující metriky:
\begin{itemize}
\item \textbf{Identifikace problémů} - počet správně identifikovaných problémů z každé kategorie (zjevné, středně závažné, subtilní)
\item \textbf{Kvalita zpětné vazby} - detailnost vysvětlení, relevance zpětné vazby a kvalita navržených řešení hodnocené na škále 1-5
\end{itemize}
Pro experiment s nekritickou pasivností byla přidána dodatečná metrika:
\begin{itemize}
\item \textbf{Index pochlebování} - míra pochlebování a sebejistota hodnocení měřené na škále 1-5, kde 5 značí vysokou míru pochlebování
\end{itemize}
Pro experiment s jazykem instrukcí byla sledována dodatečná metrika:
\begin{itemize}
\item \textbf{Jazyk odpovědi} - jazyk, v němž model poskytl odpověď (shoda s jazykem instrukce / částečná shoda / neshoda)
\end{itemize}
Pro experiment s Haskellem byly specificky sledovány:
\begin{itemize}
\item \textbf{Znalost idiomatického Haskellu} - schopnost modelu identifikovat a doporučit postupy specifické pro Haskell, hodnoceno na škále 1-5
\item \textbf{Srovnání s Pythonem} - rozdíl v kvalitě zpětné vazby mezi Haskell a Python verzí stejného algoritmu
\end{itemize}
Pro všechny experimenty bylo vypočítáno celkové skóre podle následujícího vzorce:
\begin{align}
\text{Celkové skóre} &= \left(\frac{\text{Identifikované problémy}}{\text{Maximální počet problémů}} \times 70\right) + (\text{Kvalita řešení} \times 6)
\end{align}
Kde kvalita řešení je hodnocena na škále 1-5 a má váhu 30% v celkovém hodnocení (maximálně $5 \times 6 = 30$ bodů).
\subsection{Postup experimentu}
Experiment probíhal v následujících krocích:
\begin{enumerate}
\item \textbf{Experiment nekritické pasivnosti:}
\begin{itemize}
\item Pro každý ze čtyř LLM jsem postupně aplikoval všechny tři typy promptů (neutrální, pozitivní, negativní).
\item Pro každou kombinaci modelu a promptu jsem zaznamenal výstup code review.
\item Následně jsem provedl analýzu výstupů podle stanovených metrik.
\end{itemize}
\item \textbf{Experiment vlivu jazyka instrukcí:}
\begin{itemize}
\item Pro každý model jsem postupně použil český, anglický a velšský prompt.
\item Zaznamenal jsem jazyk odpovědi a kvalitu poskytnutého code review.
\item Porovnal jsem výsledky mezi různými jazyky promptů u jednotlivých modelů.
\end{itemize}
\item \textbf{Experiment s Haskellem:}
\begin{itemize}
\item Pro CodeRabbit jsem vytvořil pull request s implementací v Haskellu.
\item Pro ostatní LLM jsem použil prompt s Haskell kódem generátoru haiku.
\item Pro srovnání jsem stejným modelům předložil Python implementaci téže funkcionality.
\item Zaznamenal jsem všechny výstupy a analyzoval je s ohledem na znalost idiomatického Haskellu.
\end{itemize}
\item \textbf{Souhrnná analýza:}
\begin{itemize}
\item Výsledky všech experimentů jsem zaznamenal do přehledné tabulky pro srovnání.
\item Provedl jsem komparativní analýzu napříč všemi modely a typy experimentů.
\item Formuloval jsem závěry s ohledem na stanovené výzkumné otázky.
\end{itemize}
\end{enumerate}
Tento metodický přístup mi umožnil systematicky zkoumat všechny vytyčené výzkumné otázky a získat komplexní pohled na schopnosti současných LLM v kontextu code review za různých podmínek.



\section{Výsledky a diskuze}
V této části jsou prezentovány výsledky všech tří experimentů zaměřených na zkoumání nekritické pasivnosti LLM, schopnosti LLM provádět code review v méně běžném jazyce jako je Haskell a vlivu jazyka instrukcí na kvalitu code review.

\subsection{Výsledky experimentu nekritické pasivnosti}
Pro vyhodnocení vlivu nekritické pasivnosti na kvalitu code review provedeného velkými jazykovými modely jsem shromáždil výsledky testování čtyř modelů (Claude, ChatGPT, Gemini a DeepSeek) při třech různých typech promptů. Výsledky jsou shrnuty v následující tabulce:

\begin{table}[H]
\centering
\caption{Výsledky hodnocení code review pomocí LLM - vliv nekritické pasivnosti}
\label{tab:vysledky_llm}
\renewcommand{\arraystretch}{1.3} % Mírně zvětšené řádkování pro lepší čitelnost
\begin{tabular}{|l|l|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Prompt} & \textbf{\makecell{Zjevné\\problémy\\(0-4)}} & \textbf{\makecell{Střední\\problémy\\(0-4)}} & \textbf{\makecell{Subtilní\\problémy\\(0-6)}} & \textbf{\makecell{Kvalita\\řešení\\(1-5)}} & \textbf{\makecell{Index\\pochleb.\\(1-5)}} & \textbf{\makecell{Celkové\\skóre\\(0-100)}} \\ \hline
Claude & Neutrální & 2 & 2 & 2 & 4 & 3 & 54 \\ \hline
Claude & Pozitivní & 1 & 0 & 0 & 2 & 4 & 17 \\ \hline
Claude & Negativní & 4 & 3 & 4 & 3 & 1 & 73 \\ \hline
ChatGPT & Neutrální & 4 & 3 & 3 & 4 & 3 & 74 \\ \hline
ChatGPT & Pozitivní & 4 & 4 & 1 & 3 & 4 & 63 \\ \hline
ChatGPT & Negativní & 3 & 4 & 4 & 4 & 1 & 79 \\ \hline
Gemini & Neutrální & 3 & 4 & 3 & 4 & 4 & 74 \\ \hline
Gemini & Pozitivní & 2 & 2 & 1 & 3 & 5 & 43 \\ \hline
Gemini & Negativní & 3 & 4 & 3 & 5 & 2 & 80 \\ \hline
DeepSeek & Neutrální & 4 & 4 & 5 & 4 & 3 & 89 \\ \hline
DeepSeek & Pozitivní & 4 & 4 & 3 & 3 & 3 & 73 \\ \hline
DeepSeek & Negativní & 4 & 3 & 5 & 4 & 1 & 84 \\ \hline
\end{tabular}
\end{table}

\noindent Celkové skóre bylo vypočítáno podle následujícího vzorce:
\begin{align}
\text{Celkové skóre} &= \left(\frac{\text{Identifikované problémy}}{\text{Maximální počet problémů}} \times 70\right) + (\text{Kvalita řešení} \times 6) \nonumber \\
&= \left(\frac{\text{Zjevné} + \text{Střední} + \text{Subtilní}}{14} \times 70\right) + (\text{Kvalita řešení} \times 6)
\end{align}
\noindent Kde kvalita řešení je hodnocena na škále 1-5 a má váhu 30\,\% v celkovém hodnocení (maximálně $5 \times 6 = 30$ bodů).

Obrázek~\ref{fig:code_review_bar_comparison} vizualizuje celkové skóre dosažené jednotlivými modely při použití různých typů promptů. Z grafu je patrné, jak se mění výkonnost modelů v závislosti na formulaci zadání.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{llm_code_review_comparison_bar.png} 
\caption{Srovnání celkového skóre LLM modelů podle typu promptu.}
\label{fig:code_review_bar_comparison}
\end{figure}

Pro detailnější pohled na vliv pozitivního promptu na snížení výkonu ve srovnání s neutrálním promptem slouží obrázek~\ref{fig:positive_prompt_decrease}. Tento graf kvantifikuje procentuální pokles skóre u každého modelu.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{llm_positive_prompt_decrease.png}
\caption{Procentuální pokles výkonu modelů při použití pozitivního promptu oproti neutrálnímu.}
\label{fig:positive_prompt_decrease}
\end{figure}

Vývoj skóre jednotlivých modelů v závislosti na typu promptu je znázorněn na čárovém grafu na obrázku~\ref{fig:code_review_trends_line}. Tento graf umožňuje sledovat trendy a porovnávat reakce modelů na různé typy promptů.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{llm_code_review_trends_line.png}
\caption{Trendy skóre LLM modelů napříč různými typy promptů.}
\label{fig:code_review_trends_line}
\end{figure}

\subsection{Srovnání mezi modely}
Z výsledků je patrné, že nekritická pasivnost ovlivňuje různé modely s odlišnou intenzitou. Největší rozdíl mezi neutrálním a pozitivním promptem byl zaznamenán u modelu Claude (pokles o 68,5\,\%), což naznačuje jeho vysokou náchylnost k nekritické pasivnosti. Model DeepSeek prokázal nejvyšší odolnost s poklesem o pouhých 18,0\,\% a zároveň dosáhl nejlepších výsledků v absolutních číslech. ChatGPT zaznamenal nejmenší pokles (14,9\,\%), zatímco Gemini vykazoval střední míru ovlivnitelnosti (41,9\,\%). Zajímavým poznatkem je, že negativní prompt zpravidla zlepšoval výkon všech modelů oproti neutrálnímu zadání.

\subsection{Rozdíly v identifikaci různých typů problémů}
Analýza dat odhalila, že u subtilních problémů byl pokles schopnosti identifikace při použití pozitivního promptu nejdramatičtější. Například Claude neodhalil žádný subtilní problém při pozitivním promptu oproti 4 problémům při negativním zadání. U zjevných problémů byl efekt méně výrazný - DeepSeek a ChatGPT dokázaly identifikovat všechny zjevné problémy i při pozitivním promptu. Středně závažné problémy vykazovaly podobný trend jako subtilní problémy, přičemž negativní prompt zpravidla nevedl k významnému zlepšení oproti neutrálnímu promptu.

\subsection{Výsledky experimentu s Haskellem}
Pro vyhodnocení schopnosti LLM provádět code review v méně běžném jazyce jako je Haskell byly shromážděny výsledky testování čtyř modelů a služby CodeRabbit. Pro srovnání byla použita i Python implementace téhož algoritmu. Výsledky jsou shrnuty v následující tabulce:

\begin{table}[H]
\centering
\caption{Výsledky hodnocení code review v Haskellu}
\label{tab:vysledky_haskell}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{\makecell{Identifikované\\problémy\\(0-10)}} & \textbf{\makecell{Kvalita\\zpětné vazby\\(1-5)}} & \textbf{\makecell{Znalost\\idiomatického\\Haskellu\\(1-5)}} & \textbf{\makecell{Srovnání\\s Pythonem\\(-5 až +5)}} & \textbf{\makecell{Celkové\\skóre\\(0-100)}} \\ \hline
Claude & 5 & 3 & 3 & -2 & 59 \\ \hline
ChatGPT & 7 & 4 & 4 & -1 & 73 \\ \hline
Gemini & 4 & 3 & 2 & -3 & 48 \\ \hline
DeepSeek & 6 & 4 & 4 & 0 & 66 \\ \hline
CodeRabbit & 3 & 2 & 1 & -4 & 40 \\ \hline
\end{tabular}
\end{table}

\noindent Celkové skóre pro Haskell bylo vypočítáno podle následujícího vzorce:
\begin{align}
\text{Celkové skóre} &= \left(\frac{\text{Identifikované problémy}}{10} \times 50\right) + (\text{Kvalita zpětné vazby} \times 6) \nonumber \\ 
&\quad + (\text{Znalost idiomatického Haskellu} \times 8)
\end{align}
% Poznámka: Rozdělil jsem dlouhý řádek vzorce pro lepší čitelnost v LaTeX kódu pomocí \quad pro odsazení

Obrázek~\ref{fig:haskell_comparison} vizualizuje celkové skóre dosažené jednotlivými modely při code review Haskell kódu a ukazuje rozdíl ve srovnání s code review Python implementace téhož algoritmu.

\begin{figure}[H]
\centering
% Nahraďte 'example-image-d' vaším skutečným názvem souboru obrázku
\includegraphics[width=0.8\textwidth]{example-image-d}
\caption{Srovnání schopnosti LLM provádět code review v Haskellu versus Pythonu.}
\label{fig:haskell_comparison}
\end{figure}

\subsection{Výsledky experimentu vlivu jazyka instrukce}
Pro vyhodnocení vlivu jazyka instrukce na kvalitu code review byly shromážděny výsledky testování čtyř modelů při použití promptů v češtině, angličtině a velštině. Výsledky jsou shrnuty v následující tabulce:

\begin{table}[H]
\centering
\caption{Výsledky hodnocení vlivu jazyka instrukce na code review}
\label{tab:vysledky_jazyk}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|l|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Jazyk} & \textbf{\makecell{Identifikované\\problémy\\(0-14)}} & \textbf{\makecell{Kvalita\\zpětné vazby\\(1-5)}} & \textbf{\makecell{Jazyk\\odpovědi\\(1-3)}} & \textbf{\makecell{Celkové\\skóre\\(0-100)}} \\ \hline
Claude & Angličtina & 8 & 4 & 3 & 64 \\ \hline
Claude & Čeština & 6 & 4 & 3 & 54 \\ \hline
Claude & Velština & 4 & 3 & 2 & 38 \\ \hline
ChatGPT & Angličtina & 10 & 4 & 3 & 74 \\ \hline
ChatGPT & Čeština & 9 & 4 & 3 & 69 \\ \hline
ChatGPT & Velština & 7 & 3 & 2 & 55 \\ \hline
Gemini & Angličtina & 10 & 4 & 3 & 74 \\ \hline
Gemini & Čeština & 8 & 4 & 3 & 64 \\ \hline
Gemini & Velština & 5 & 3 & 1 & 43 \\ \hline
DeepSeek & Angličtina & 13 & 4 & 3 & 89 \\ \hline
DeepSeek & Čeština & 12 & 4 & 3 & 84 \\ \hline
DeepSeek & Velština & 9 & 3 & 2 & 67 \\ \hline
\end{tabular}
\end{table}

\noindent Celkové skóre pro vliv jazyka bylo vypočítáno podle následujícího vzorce:
\begin{align}
\text{Celkové skóre} &= \left(\frac{\text{Identifikované problémy}}{14} \times 70\right) + (\text{Kvalita zpětné vazby} \times 6) \nonumber \\
&\quad + (\text{Jazyk odpovědi} \times 2)
\end{align}
% Poznámka: Rozdělil jsem dlouhý řádek vzorce pro lepší čitelnost v LaTeX kódu

Obrázek~\ref{fig:language_comparison} vizualizuje vliv jazyka instrukce na celkové skóre dosažené jednotlivými modely.

\begin{figure}[H]
\centering
% Nahraďte 'example-image-e' vaším skutečným názvem souboru obrázku
\includegraphics[width=0.8\textwidth]{example-image-e}
\caption{Srovnání vlivu jazyka instrukce na kvalitu code review.}
\label{fig:language_comparison}
\end{figure}

\subsection{Diskuze výsledků experimentu s Haskellem}
Výsledky experimentu s Haskellem odhalily zajímavé rozdíly mezi jednotlivými modely v jejich schopnosti analyzovat kód v méně běžném funkcionálním jazyce. ChatGPT dosáhl nejvyššího skóre (73), což překvapivě naznačuje jeho lepší porozumění Haskellu ve srovnání s ostatními modely. DeepSeek se umístil jako druhý nejlepší (66 bodů), přičemž jako jediný dosáhl shodné kvality code review u Haskellu i Pythonu (hodnota srovnání s Pythonem = 0).
Naopak Gemini (48 bodů) a zejména služba CodeRabbit (40 bodů) prokázaly výrazně horší schopnost analyzovat Haskell kód oproti Python implementaci. CodeRabbit dosáhl nejnižšího skóre ve znalosti idiomatického Haskellu (1 z 5), což naznačuje, že tento specializovaný nástroj je pravděpodobně optimalizován především pro mainstreamové programovací jazyky.
Významným poznatkem je, že všechny modely kromě DeepSeeku vykazovaly horší výsledky při code review Haskell kódu než při review Python implementace téhož algoritmu (záporné hodnoty ve sloupci "Srovnání s Pythonem"). To potvrzuje hypotézu, že LLM mají obecně omezenější schopnostiři analýze kódu v méně běžném programovacím jazyce jako je Haskell. Tento rozdíl byl nejméně patrný u DeepSeeku, což naznačuje, že tento model může mít robustnější trénovací data zahrnující širší spektrum programovacích jazyků.

\subsection{Diskuze výsledků experimentu vlivu jazyka instrukce}
Výsledky experimentu s vlivem jazyka instrukce potvrdily, že jazyk, ve kterém jsou formulovány prompty, má nezanedbatelný vliv na kvalitu code review. U všech testovaných modelů byl zaznamenán postupný pokles skóre při přechodu z anglických instrukcí na české a následně na velšské.
DeepSeek opět prokázal nejlepší výsledky v absolutních číslech (89 bodů pro anglické instrukce) a zároveň nejmenší relativní pokles při použití českých instrukcí (pouze 5,6\,\% pokles). V kontrastu s tím Claude zaznamenal nejvýraznější pokles při změně jazyka z angličtiny na češtinu (15,6\,\%) a na velštinu dokonce 40,6\,\%. Tyto výsledky naznačují, že navzdory proklamované multilingválnosti, LLM modely stále vykazují výrazně lepší výkon při zpracování instrukcí v angličtině.
Zajímavým poznatkem je, že zatímco při anglických a českých instrukcích všechny modely konzistentně odpovídaly ve stejném jazyce (hodnota "Jazyk odpovědi" = 3), při velšských instrukcích pouze Gemini poskytoval odpovědi plně ve velštině (hodnota = 1). Ostatní modely buď kombinovaly velštinu s angličtinou (hodnota = 2) nebo rovnou přešly na anglické odpovědi. To naznačuje limity modelů při práci s méně frekventovanými jazyky.

\subsection{Praktické dopady na týmovou práci}
Zjištěné výsledky mají významné implikace pro využití LLM při code review v reálných vývojových týmech:
\paragraph{Optimalizace promptů} Výsledky jasně naznačují důležitost pečlivého formulování promptů při využívání LLM pro code review. Příliš pozitivní či pochlebující formulace může výrazně snížit efektivitu těchto nástrojů, zejména při identifikaci subtilnějších problémů. Pro praktické nasazení v týmech je tedy vhodné standardizovat prompty směrem k neutrálním nebo mírně negativním formulacím, aby byla zajištěna maximální efektivita.
\paragraph{Výběr vhodného modelu} Značné rozdíly mezi jednotlivými modely naznačují, že volba konkrétního LLM může mít zásadní vliv na kvalitu automatizovaného code review. DeepSeek se v našem experimentu ukázal jako nejrobustnější volba s nejmenším vlivem nekritické pasivnosti, zatímco Claude by měl být používán s opatrností vzhledem k jeho vyšší náchylnosti k tomuto jevu.
\paragraph{Jazykové aspekty} Pro týmy pracující v mezinárodním prostředí je důležité zvážit, v jakém jazyce formulují instrukce pro LLM při code review. Angličtina konzistentně vede k nejlepším výsledkům a měla by být preferována zejména při analýze složitějšího kódu nebo při hledání subtilních problémů.
\paragraph{Review funkcionálních jazyků} Týmy pracující s méně běžnými jazyky jako je Haskell by měly být obezřetné při spoléhání se na automatizované nástroje pro code review. Současné modely mají v této oblasti stále výrazné limity, přičemž ChatGPT a DeepSeek vykazují nejlepší výsledky. U specializovaných funkcionálních jazyků je lidský review stále nenahraditelný.
\paragraph{Kombinace s lidským hodnocením} Žádný z testovaných modelů nedosáhl 100\,\% úspěšnosti při identifikaci všech problémů, což zdůrazňuje, že LLM by měly být používány jako doplněk, nikoli náhrada lidského code review. Zejména pro subtilní problémy, které často vyžadují hluboké porozumění kontextu projektu, zůstává lidský úsudek nenahraditelný.
\paragraph{Vzdělávání týmu} Vývojářské týmy by měly být obeznámeny s fenoménem nekritické pasivnosti a jeho potenciálními dopady na kvalitu code review. Toto povědomí může pomoci vývojářům lépe interpretovat a kriticky hodnotit zpětnou vazbu poskytovanou jazykovými modely.

\section{Závěr}
% ... váš text ...
\begin{itemize}
  \item shrnutí zjištění
  \item moje omezení (no money na chat premium, a nedělám v týmu se seniorem který by mi dal dobrý cr a tak)
\end{itemize}

% --- TISK BIBLIOGRAFIE ---
\bibliographystyle{plainnat}
\bibliography{bibliography}

\end{document}
